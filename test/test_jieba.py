from src.core.tokenizer import JiebaTokenizer
import yaml

config_path = "./config/bm25.yaml"

with open(config_path, 'r') as f:
    config = yaml.safe_load(f)


tokenizer = JiebaTokenizer(
    user_dict_path=config['tokenizer'].get('user_dict_path', None),
    stopwords_path=config['tokenizer'].get('stopwords_path', None),
    normalize_config=config['tokenizer'].get('normalize', {}),
    filter_config=config['tokenizer'].get('filter', {})
)
text = "ä»Šå¤©çœŸçš„å¾ˆé–‹å¿ƒğŸ˜‚ğŸ˜‚!!! å¯Œé‚¦ä¿éšªå¾ˆå¼·ğŸš€ğŸš€"

tokens = tokenizer.tokenize(text)